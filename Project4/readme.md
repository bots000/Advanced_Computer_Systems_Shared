# Introduction

For this project, the students were tasked with implementing a dictionary codec using multithreading; the real-world applications of this are data compression and sped search/scan operations.  All three of these functions are implemented in this project.  This project demonstrates the use of a B tree and of SIMD instructions.  This report will detail how this was programmed, how to run the programs, experimental results gathered from running the programs, and the analysis of these results.

# Program Explanation

There are three main components of this project; the encoding, the search, and the B tree.  First, the process taken to encode the dictionary will be explained.  This was achieved using multithreading.  The main function will loop through the raw data provided, and dispatch chunks of it (defined by a set number of data items to be dispatched, referred to as a per_thread count) to the thread worker function.  This function goes through the dispatched data chunks item by item, creates a hashed value for each raw data item, and adds it to a local dictionary (c++ map).  These hashed values are generated using a built-in c++ hashing function, and are representative of the encoded values of the raw data.  Once this local dictionary is populated with all of the dispatched raw data items and the corresponding hashed (encoded) values, it is returend by the worker thread function.  When retrieved by the main function, this local dictionary is merged with a "master" dictionary, which will hold the raw data/encoded data parings for all raw data items.

Once all data is encoded through the process detailed above, the raw data/encoded data pairings are added to a B+ tree.  The foundation of the code used to create such a tree was found at https://github.com/solangii/b-plus-tree, and proper credit is given to the author of this code.  Group members of this project edited this code so that it could also hold an encoded value, but was still sorted on the raw data value.  Additionally, functions were added/deleted as needed for the desired functionality for this project.  Key functions include .insert(data, encoded_data), .find_index(array, find_data, len_of_array), and .range_search(starting_value, ending_value, result_data, result_array_length).  These aid in populating the B tree, finding if an element exists with it, and finding all elements within a certain range.  It is important to note that these find/scan functions take in the raw data as input, not encoded data.  Delete functions were not implemented, as this project uses a static B-tree (after all ementas are inserted), not a dynamic one.  The purpose of implementing the B-tree was to speed up the search/scan operations on the encoded dictionary; after its creation, finding if an element exists and finding all elements within a certain range (the two processes detailed by the prject instructions), become as trivial as calling the two above functions.

SIMD execution is relatively straightforward. The function findIndices(array, query_value, length) will run through the encoded array 1 time and return a vector of all the indices in which the query value appears. The function is configured for 64 bit encodings, though with the primitive support available, it would be simple to adapt to 32, 16, or 8 bit encodings. The high level flow of the function is as follows: 
1. Load a set of encodings into AVX registers (4 encodings per register)
2. For each set, do a 64 bit compare with compareVec (query value)
3. From each mask vector, extract the mask into an int.
4. If the mask is nonzero (meaning there was a match), extract all indices
5. Store indices
6. Repeat until all blocks have been compared.

A major concern for efficient SIMD execution was the index extraction. Simple approaches may bottleneck the execution by checking all possible mask configurations. The most efficent method found was to characterize the mask by counting the trailing zeroes. When the mask is nonzero, we know there is an index to extract. The function _tzcnt_u32(mask) was used to retrieve the number of trailing zeros, which could then be used to calculate the index. To account for cases where there are multiple indices in one mask, bitmasking is then used to place a 0 where the first 1 is. The process continues until the mask is equal to 0, therefore only performing index extraction when necessary.


The output file generated by this program is a text file that contains all of the leaf nodes of the B tree - that is, the raw data/encoded data pairs, in order in alphabetical order of the raw data.  It is a printed out version of the encoded dictionary.  After this, the encoded data is printed - it is in the same order as the raw data, and is just a direct translation.  The output text file to write this to will be specified as a command line parameter.

# How to Run

The code for this project is split between two files: hashed_b_tree_test.cpp and BPlusTree.h.  BPlusTree.h contains the code needed to include B Tree functionality, and hashed_b_tree_test.cpp is the main file; it does the multithreading encoding, search/scan operations via SIMD instructions, writes to the output file, etc.  BPlusTree.h is imported in hashed_b_tree_test.cpp.  In order to compile this code, the following command should be used:

g++ -march=native -mavx hashed_b_tree_test.cpp -o outfile

The other resource needed to run this the raw column data.  This data should be in the form of a text file, with each element of data on its own individual line (separated by a newline).  The program is not configured to be able to parse any other data format.  This input file is taken as a command line argument.  The remaining two command line arguments include the number of threads to be used and the number of data items to encode per thread.  These are taken as command line arguments as they are both variables that can directly affect performance, so they were made easily adjustable for convinent testing.  An example of running the file with input file in.txt, output file out.txt, 10 threads and 100 data items encoded per thread is shown below.

./outfile 10 in.txt out.txt 100

The SIMD search is programmed to only use 8 compare registers and 1 query value at a time in order to minimize page faults when running through the array. It is possible that in other applications it may be more optimal to have several query values at once and only 1 compare register in use for cases where the prefix search is more heavily used. However, we felt it was more important to emphasize the case of 1 query value.

For the SIMD search and prefix tests, 4 csv files are created by default containing the results. 

# Experimental Results
## Encoding Speed Results

Time was measured between the time when the first raw data item is retrieved to when final encoding of all data items were complete.  This was measured as a function of the number of threads used and the number of data items encoded per thread.  These results are shown in the figure below.

![Encoding Speed Heamap](https://github.com/bots000/Advanced_Computer_Systems_Shared/blob/daaa4bdfd9c6127a79a717deb1c202cbb93c4324/Project4/plots/encoding_heatmap.png)

## Single Data Item Search Speed Results

In order to test the efficiency of the SIMD search implementation, several items were chosen at random from Column.txt. Only the time to find all indices given the hash was tested. Shown below are the results from the test.

![SIMD Single Query Results](https://raw.githubusercontent.com/bots000/Advanced_Computer_Systems_Shared/main/Project4/plots/SIMD_MatchVsTime.png)

## Prefix Scan Speed Results

Because the prefix search consists of two distinct elements, fetching encodings and searching the array for indices, they are tested separately. To test different prefixes, items were taken from Column.txt and trimmed to 3-5 characters. Below are the results of 10 tests.

![Prefix Encoding Search Results](https://raw.githubusercontent.com/bots000/Advanced_Computer_Systems_Shared/main/Project4/plots/Pre_MatchVsTime.png)

![Prefix Index Search Results](https://raw.githubusercontent.com/bots000/Advanced_Computer_Systems_Shared/main/Project4/plots/Pre_IndexVsTime.png)

# Analysis
## Encoding Speed Analysis

Viewing the encoding speed heatmap, the encoding runtime can be viewed as a function of the number of threads used and the number of data items encoded per thread.   Excluding outliers, a general pattern can be observed.  As the number of threads and the number of data items encoded per thread increases, the overall runtime decreases.  Each of these are consistent with expectations.  From Project 1 in this course, it was shown that increasing thread count, to a point, will increase efficiency.  This is also seen in this project's results.  The optimal number of threads seems to be between around 125.  Up until this point, the runtime decreaes with more threads.  Beyond this point, more threads seems to hurt the code's efficiency.  This is due to the fact that the generation/handling of the threads becomes the cause of the latency, rather than the running of the thread function.  As previously mentioned, the runtime also, gnerally, decreased as the number of data items encoded per thread increased.  Similar to the trend seen in the number of threads used, too many data items processed per thread resulted in impaired performance.

It became clear that there is a trade-off between these two variables; optimal performance comes when a thread's results are gathered exactly as it finishes running its thread function.  It is of course difficult to determine this exaclty (without an exhaustive number of runs), but it can be observed that this optimal point occurs around 125 threads and 800 data items per thread.

## Single Data Item Search Speed Results

Shown above, the results show that even for much higher match quantities, the execution time remained around 0.275 seconds. This suggests that the key limiting factor was the ability to move through the array, not the ability to extract indices. This does make sense, given that even for the cases of the highest number of matches, the indices make up a small fraction of the full data. If the inverse was true, it is likely that the relationship would be much more linear between matches and time. Given the current results, if we wanted to speed up the index finding, we would need more AVX units from other cores instead of just 1. Alternatively, switching to a lower sized encoding (32/16/8) would allow for much faster execution by increasing the number of checks per loop. This would come at the cost of using more complicated encoding schemes to prevent overlap. For many cases though, it is more favorable to sacrifice initial encoding time to gain search efficiency.

## Prefix Scan Speed Results

The results from the prefix search highlight several important aspects of our implementation. First, for the prefix encoding search, the worst case is somewhat decoupled from the amount of expected matches. The reason for the spike in time despite the low matches is likely due the range spanning the boundary of a higher level parent, meaning that more items need to be scanned. A potential solution is a rework of the tree that maps its neighbors together, allowing for an easier traversal across layers. Second, the SIMD query scales strongly with the number of matches. This is because it is doing a single query value at a time, greatly reducing the efficiency. If this were to be reimplemented with an emphasis on ensuring the array is only iterated through once, it is very likely that there would be a significant speedup. 

# Conclusion



